{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Oe1IhZnSJR4B",
        "ZxENO9whjVOs"
      ],
      "authorship_tag": "ABX9TyOIp03tXNRxNZX/612bZwqb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YASsMeN1997/Web_Scrapping/blob/main/web_scrap_sel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# libraries"
      ],
      "metadata": {
        "id": "WVylU4jtkE7E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5tsf_nJr84Uk"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import bs4\n",
        "from urllib.request import urlopen\n",
        "import time\n",
        "import re\n",
        "import time\n",
        "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
        "#Download the libraries needed to use Selenium\n",
        "\n",
        " ### note: uncomment this part when you run the code ###\n",
        "\n",
        "'''!apt-get update\n",
        "\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium~=4.4.3\n",
        "!pip install threaded\n",
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "!apt install unzip\n",
        "!pip install streamlit_lottie\n",
        "!pip install streamlit-option-menu\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "#Settings for using the driver without a UI\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "options.add_argument(\"start-maximized\")\n",
        "options.add_argument(\"disable-infobars\")\n",
        "options.add_argument(\"--disable-extensions\")\n",
        "driver = webdriver.Chrome('chromedriver',options=options)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# streamlit trial\n"
      ],
      "metadata": {
        "id": "7tf1lxc1vo7N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wpm6QQu7mLjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''!mkdir -p /drive/ngrok-ssh\n",
        "%cd /drive/ngrok-ssh\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -O ngrok-stable-linux-amd64.zip\n",
        "!unzip -u ngrok-stable-linux-amd64.zip\n",
        "!cp /drive/ngrok-ssh/ngrok /ngrok\n",
        "!chmod +x /ngrok'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "vpMP-Fmhz6D3",
        "outputId": "9070dcf0-2ae0-47d7-9e9b-792e596c77e9"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!mkdir -p /drive/ngrok-ssh\\n%cd /drive/ngrok-ssh\\n!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -O ngrok-stable-linux-amd64.zip\\n!unzip -u ngrok-stable-linux-amd64.zip\\n!cp /drive/ngrok-ssh/ngrok /ngrok\\n!chmod +x /ngrok'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r2290k4tAeZ-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import requests\n",
        "import numpy as np\n",
        "from streamlit_lottie import st_lottie\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import bs4\n",
        "from urllib.request import urlopen\n",
        "import time\n",
        "import re\n",
        "import time\n",
        "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "#Settings for using the driver without a UI\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "options.add_argument(\"start-maximized\")\n",
        "options.add_argument(\"disable-infobars\")\n",
        "options.add_argument(\"--disable-extensions\")\n",
        "driver = webdriver.Chrome('chromedriver',options=options)\n",
        "\n",
        "site = \"Wuzzuf\"\n",
        "job =\"Machine Learning\"\n",
        "num_jobs = 5\n",
        "st.set_page_config(page_title=\"My Web_Scrap Page\", page_icon=\":tada:\", layout=\"wide\")\n",
        "def load_lottieurl(url):\n",
        "    r = requests.get(url)\n",
        "    if r.status_code != 200:\n",
        "        return None\n",
        "    return r.json()\n",
        "\n",
        "# ---- LOAD ASSETS ----\n",
        "lottie_coding = load_lottieurl(\"https://assets5.lottiefiles.com/packages/lf20_fcfjwiyb.json\")\n",
        "\n",
        "\n",
        "# ---- HEADER SECTION ----\n",
        "with st.container():\n",
        "    st.subheader(\"Hi, I am Yassmen :wave:\")\n",
        "    st.title(\"Electronics and Communcation Engineer\")\n",
        "    st.write(\n",
        "        \"In this app we will scrap jobs from LinkedIn and Wuzzuf websites, let's get it started :boom:\"\n",
        "    )\n",
        "    st.write(\"[Reach me >](https://www.linkedin.com/in/yassmen-youssef-48439a166/)\")\n",
        "\n",
        "with st.container():\n",
        "    st.write(\"---\")\n",
        "    left_column, right_column = st.columns(2)\n",
        "\n",
        "    with right_column:\n",
        "        st_lottie(lottie_coding, height=300, key=\"coding\")\n",
        "\n",
        "import streamlit as st\n",
        "from streamlit_option_menu import option_menu\n",
        "\n",
        "with st.sidebar:\n",
        "    selected = option_menu(\"Main Menu\", [\"select website\", 'search job','numbers of jobs'], icons=['linkedin', 'search','123'], menu_icon=\"cast\", default_index=1)\n",
        "    \n",
        "webs =[\"Wuzzuf\",\"Linkedin\"]\n",
        "jobs =[\"Machine Learning\",\"AI Engineer\",\"Data Analysis\",\"Software Testing\"]\n",
        "nums = np.arange(1,1000)\n",
        "\n",
        "if selected == \"select website\":\n",
        "  site = st.sidebar.selectbox(\"select one website\", webs)\n",
        "if selected == \"search job\":\n",
        "  job = st.sidebar.selectbox(\"select one job\", jobs)\n",
        "if selected == \"numbers of jobs\":\n",
        "  num_jobs = st.sidebar.selectbox(\"select num of jobs you want to scrap\", nums)\n",
        "\n",
        "if site ==\"Wuzzuf\":\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWHL5tY8unU_",
        "outputId": "db0a9c93-6260-4826-94ba-3ed68d31fc2d"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhxD-qkPvFdA",
        "outputId": "578cfb0d-8727-492c-e064-2f83daa437b0"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\tnohup.out  sample_data\tWUZZUF_scrapping.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyngrok import ngrok \n",
        "!ngrok authtoken \"2FpXLfPnzvs4s2S9knmtVCnmWHX_5kFivZ7E1NjyGvnBQozpj\"\n",
        "!nohup streamlit run app.py & \n",
        "\n",
        "url = ngrok.connect(port = 8501)\n",
        "url"
      ],
      "metadata": {
        "id": "uRc56rhN0xrW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fb13bf8-aef0-492a-cd87-0c66f0d35c03"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n",
            "nohup: appending output to 'nohup.out'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://b000-34-125-107-166.ngrok.io\" -> \"http://localhost:80\">"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run --server.port 80 app.py >/dev/null\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ_j8kNmCjwM",
        "outputId": "6ae70d52-a205-4359-9cb3-a6c3eef77757"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-10-09 04:58:58.409 INFO    numexpr.utils: NumExpr defaulting to 2 threads.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()\n",
        "\n"
      ],
      "metadata": {
        "id": "RPm-m_VO1_hT"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U5a9ykTdo1Uo"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEW wuzzuf"
      ],
      "metadata": {
        "id": "Oe1IhZnSJR4B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mk1oVJ-N88Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Wuzzuf_scrapping(job_type , job_num):\n",
        "  job1 = job_type.split(\" \")[0]\n",
        "  job2 = job_type.split(\" \")[1]\n",
        "  link1 = 'https://wuzzuf.net/search/jobs/?a=navbl&q='+job1+'%20'+job1\n",
        "  title = []\n",
        "  location = []\n",
        "  country = []\n",
        "  job_description = []\n",
        "  Job_Requirements =[]\n",
        "  company_name = []\n",
        "  links = []\n",
        "  Jop_type = []\n",
        "  Career_Level = []\n",
        "  company_logo = []\n",
        "  Job_Categories = []\n",
        "  Skills_And_Tools = []\n",
        "  Experience_Needed =[]\n",
        "  post_time = []\n",
        "  Title = []\n",
        "  pages_num = np.ceil(job_num/15)\n",
        "\n",
        "\n",
        "  for i in range(int(pages_num) ):\n",
        "    link_new = link1 +'&start='+str(i)\n",
        "    data  = requests.get(link_new)\n",
        "    soup  = BeautifulSoup(data.content)\n",
        "    Title = soup.find_all('h2' , {'class': 'css-m604qf'})\n",
        "\n",
        "# to get the info about jobs\n",
        "\n",
        "    for x in range(0,len(Title)):\n",
        "      t = re.split('\\(|\\-',Title[x].find('a').text)\n",
        "      title.append(t[0].strip())\n",
        "      loc = re.split(',' , soup.find_all('span' , {'class': 'css-5wys0k'})[x].text)\n",
        "      r = \"\"\n",
        "      for i in range(len(loc[:-1])):\n",
        "        r= r+ ', ' +loc[:-1][i].strip()\n",
        "      location.append(r.replace(',', '', 1).strip())\n",
        "      country.append(loc[-1].strip())\n",
        "      links.append('https://wuzzuf.net' + Title[x].find('a').attrs['href'])\n",
        "      m = \" \".join(re.findall(\"[a-zA-Z\\d+]+\", (soup.find_all('div' , {'class': 'css-d7j1kk'})[x].find('a').text)))\n",
        "      company_name.append(m)\n",
        "      c = soup.find_all('div' ,{'class':'css-1lh32fc'})[x].find_all('span')\n",
        "      if len(c) ==1:\n",
        "        Jop_type.append(c[0].text)\n",
        "      else:\n",
        "        n =[]\n",
        "        for i in range(len(c)):\n",
        "          n.append(c[i].text)\n",
        "        Jop_type.append(n)\n",
        "      n =soup.find_all('div' ,{'class':'css-y4udm8'})[x].find_all('div')[1].find_all(['a','span'])\n",
        "      Career_Level.append(n[0].text)\n",
        "      n =soup.find_all('div' ,{'class':'css-y4udm8'})[x].find_all('div')[1].find_all(['a','span'])\n",
        "\n",
        "      yy = n[1].text.replace('Â·',' ').strip()\n",
        "      yy = re.findall('[0-9-+]*',yy)\n",
        "      y1 =\"\"\n",
        "      for i in range(len(yy)):\n",
        "      \n",
        "        if any(yy[i]):\n",
        "          y1 = y1+yy[i]\n",
        "      if y1 != \"\":\n",
        "        Experience_Needed.append(y1)\n",
        "      else:\n",
        "        Experience_Needed.append(\"Not Specified\")\n",
        "      time = (soup.find_all('div' ,{'class':'css-d7j1kk'}))[x].find('div')\n",
        "      post_time.append(time.text)\n",
        "      \n",
        "# to get the logo of the company\n",
        "    \n",
        "      data1  = requests.get(links[x])\n",
        "      soup1 = BeautifulSoup(data1.content)\n",
        "      company_logo.append(soup1.find_all('meta',{'property':\"og:image\"})[0]['content'])\n",
        "      #time.sleep(4)\n",
        "\n",
        "\n",
        " # get Job_Categories , Skills_And_Tools , job_description , and job_requirements from urls\n",
        "      driver = webdriver.Chrome('chromedriver',options=options)\n",
        "      #driver.implicitly_wait(10)\n",
        "      driver.get(links[x])\n",
        "      Job_Categories.append(driver.find_element(By.XPATH ,'//*[@id=\"app\"]/div/main/section[2]/div[5]').text.split(\"\\n\")[1:])\n",
        "      Skills_And_Tools.append(driver.find_element(By.XPATH ,'//*[@id=\"app\"]/div/main/section[2]/div[6]').text.split(\"\\n\")[1:])\n",
        "      job_description.append(driver.find_element(By.XPATH ,'//*[@id=\"app\"]/div/main/section[3]').text.split(\"\\n\")[1:])\n",
        "      #post_time.append(' '.join(driver.find_element(By.XPATH , '//*[@id=\"app\"]/div/main/section[1]/div/span').text.split(\" \")[1:]))\n",
        "      #Career_Level.append(' '.join(driver.find_element(By.XPATH , '//*[@id=\"app\"]/div/main/section[2]/div[2]/span[2]/span').text.split(\" \")[:]))\n",
        "      all =driver.find_elements(By.XPATH ,'//*[@id=\"app\"]/div/main/section[4]/div')\n",
        "\n",
        "      dict_other = {}\n",
        "   \n",
        "      if len(all[0].find_elements(By.TAG_NAME ,\"p\")) != 0:\n",
        "        k = all[0].find_elements(By.TAG_NAME ,\"p\")\n",
        "        c = all[0].find_elements(By.TAG_NAME ,\"ul\")\n",
        "        for i in range(len(all[0].find_elements(By.TAG_NAME ,\"p\"))):\n",
        "          dict_other[k[i].text] = c[i].text\n",
        "        Job_Requirements.append(dict_other)\n",
        "\n",
        "      else:\n",
        "        Job_Requirements.append(all[0].find_elements(By.TAG_NAME ,\"ul\")[0].text)\n",
        "\n",
        "\n",
        "#  create data frame to combine all together\n",
        "\n",
        "  df = pd.DataFrame({'Title' : title , 'Location' : location ,'country':country,'URLs':links ,'Company_Name' : company_name,'Career_Level':Career_Level,'post_time':post_time,'Experience_Needed':Experience_Needed,'Company_Logo':company_logo,\"Job_Categories\":Job_Categories , \"Skills_And_Tools\":Skills_And_Tools , \"job_description\":job_description,\"Job_Requirements\":Job_Requirements})\n",
        "  \n",
        "  df[:job_num].to_excel('WUZZUF_scrapping.xlsx',index=False,encoding='utf-8')\n",
        "  return df[:job_num]"
      ],
      "metadata": {
        "id": "a-8PrzypFdBM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Y7gAdpIIYgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEW LINKEDIN"
      ],
      "metadata": {
        "id": "ZxENO9whjVOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LINKEDIN_Scrapping(job_search , num_jobs):\n",
        "  job1 = job_search.split(\" \")[0]\n",
        "  job2 = job_search.split(\" \")[1]\n",
        "\n",
        "  link1 = 'https://www.linkedin.com/jobs/search?keywords='+job1 +'%20' +job2 +'&location=&geoId=&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0'\n",
        "  \n",
        "  # FIRST get main informations about jobs\n",
        "\n",
        "  title = []\n",
        "  location = []\n",
        "  country = []\n",
        "  company_name = []\n",
        "  post_time = []\n",
        "  links =[]\n",
        "  # get the specific numbers of jobs\n",
        "  l1 = \"\"\n",
        "  ll =\"\"\n",
        "  driver = webdriver.Chrome('chromedriver',options=options)\n",
        "  driver.get(link1)\n",
        "  SCROLL_PAUSE_TIME = 0.5\n",
        "  while True :\n",
        "    l1 = driver.find_elements(By.XPATH,'//*[@id=\"main-content\"]/section[2]/ul/li[*]/div')\n",
        "    ll= driver.find_elements(By.XPATH ,'//*[@id=\"main-content\"]/section[2]/ul/li[*]/div/a') \n",
        "\n",
        "    if len(l1) >= num_jobs:\n",
        "      break\n",
        "    time.sleep(3)\n",
        "    # Get scroll height\n",
        "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    while True:\n",
        "        \n",
        "        # Scroll down to bottom\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "\n",
        "        # Wait to load page\n",
        "        time.sleep(SCROLL_PAUSE_TIME)\n",
        "\n",
        "        # Calculate new scroll height and compare with last scroll height\n",
        "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        if new_height == last_height:\n",
        "            break\n",
        "        last_height = new_height\n",
        "        \n",
        "    options.add_argument(\"window-size=1200x600\")\n",
        "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"main-content\"]/section[2]/button'))).click()\n",
        "\n",
        "    time.sleep(2)\n",
        "    \n",
        "\n",
        "\n",
        "  l2 = l1[:num_jobs]\n",
        "\n",
        "  for info in l2:   \n",
        "    info_tot = info.text.split(\"\\n\")\n",
        "    if len(info_tot)==5:\n",
        "      title.append(info_tot[1])\n",
        "      location.append(info_tot[3])\n",
        "      company_name.append(info_tot[2])\n",
        "      post_time.append(info_tot[4])\n",
        "    else:\n",
        "      title.append(info_tot[1])\n",
        "      location.append(info_tot[3])\n",
        "      company_name.append(info_tot[2])\n",
        "      post_time.append(info_tot[5])\n",
        "\n",
        "  # get links for jobs\n",
        "  l3 = ll[:num_jobs]\n",
        "  for i in l3:\n",
        "    links.append(i.get_attribute('href'))\n",
        "  \n",
        "  df_ml = pd.DataFrame({'Title' : title , 'Location' : location ,'URLs':links ,'Company_Name' : company_name ,'post_time':post_time})\n",
        "\n",
        "\n",
        "  links = df_ml.URLs\n",
        "\n",
        "\n",
        "    # GET DESCRIPTION AND LOGO \n",
        "  def all_description_LOGO(urls):\n",
        "    description =[]\n",
        "    LOGO =[]\n",
        "    for link in urls:         \n",
        "      driver = webdriver.Chrome('chromedriver',options=options)\n",
        "      driver.get(link)\n",
        "      options.add_argument(\"window-size=1200x600\")\n",
        "      WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"main-content\"]/section[1]/div/div[1]/section[1]/div/div/section/button[1]'))).click()\n",
        "\n",
        "      K = driver.find_element(By.XPATH,'//*[@id=\"main-content\"]/section[1]/div/section[2]/div/a/img')\n",
        "      LOGO.append(K.get_attribute('src'))\n",
        "      time.sleep(3)\n",
        "      t = driver.find_element(By.XPATH ,'//*[@id=\"main-content\"]/section[1]/div/div[1]/section[1]/div/div/section/div')\n",
        "      t_reverse=t.text[::-1]\n",
        "\n",
        "      if t_reverse[:9] ==\"erom wohs\":\n",
        "        l = len(t.text)\n",
        "        description.append(t.text[:l-9].split(\"\\n\"))\n",
        "      else:\n",
        "        description.append(t.text.split(\"\\n\"))\n",
        "    df_ml = pd.DataFrame({'all_about_job' : description ,'company_logo':LOGO})\n",
        "\n",
        "    return df_ml\n",
        "\n",
        "  # apply desc. and logo function\n",
        "    E = all_description_LOGO(LINKS)\n",
        "\n",
        "  # other info function\n",
        "  def other(urls):\n",
        "    frames =[]\n",
        "    for url in urls:\n",
        "      data1 = requests.get(url)\n",
        "      soup1 = BeautifulSoup(data1.content)\n",
        "      j =  soup1.find('ul' , {'class': 'description__job-criteria-list'})\n",
        "      time.sleep(4)\n",
        "      jj=j.find_all('h3')\n",
        "      dic ={}\n",
        "      for i in range(len(jj)):\n",
        "        dic[jj[i].text.replace('\\n',' ').strip()] = j.find_all('span')[i].text.replace('\\n',' ').strip()\n",
        "      output = pd.DataFrame()\n",
        "      output = output.append(dic, ignore_index=True) \n",
        "      frames.append(output)\n",
        "    result = pd.concat(frames)\n",
        "    return result\n",
        "\n",
        "  # apply Other function\n",
        "  df = other(links)\n",
        "  df.fillna('Not_Found',inplace= True)\n",
        "  df.reset_index(inplace=True, drop=True)\n",
        " \n",
        " # combine all together\n",
        "  result = pd.concat([df_ml,E, df ], axis=1)\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "ikbQ4WkgF7gH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M3zd85o4BFkv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}